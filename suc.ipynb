{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGEPQyCv0GgY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MP3sqKGu4h-I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1f9c4b-7fc3-4f97-e66f-97afd76da580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'exllamav2'...\n",
            "remote: Enumerating objects: 8168, done.\u001b[K\n",
            "remote: Counting objects: 100% (3421/3421), done.\u001b[K\n",
            "remote: Compressing objects: 100% (983/983), done.\u001b[K\n",
            "remote: Total 8168 (delta 2515), reused 2478 (delta 2438), pack-reused 4747 (from 2)\u001b[K\n",
            "Receiving objects: 100% (8168/8168), 21.95 MiB | 14.59 MiB/s, done.\n",
            "Resolving deltas: 100% (5887/5887), done.\n",
            "/content/exllamav2\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
            "Collecting ninja (from -r requirements.txt (line 2))\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (0.45.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (75.2.0)\n",
            "Collecting fastparquet (from -r requirements.txt (line 5))\n",
            "  Downloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.18.0)\n",
            "Requirement already satisfied: websockets in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (15.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (2024.11.6)\n",
            "Collecting numpy~=1.26.4 (from -r requirements.txt (line 12))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m860.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.21.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (13.9.4)\n",
            "Requirement already satisfied: pillow>=9.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: cramjam>=2.3 in /usr/local/lib/python3.11/dist-packages (from fastparquet->-r requirements.txt (line 5)) (2.9.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from fastparquet->-r requirements.txt (line 5)) (2025.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastparquet->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.2.0->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.2.0->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->-r requirements.txt (line 13)) (0.30.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->-r requirements.txt (line 14)) (3.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (4.67.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 14)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.2.0->-r requirements.txt (line 6)) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers->-r requirements.txt (line 13)) (2025.1.31)\n",
            "Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastparquet-2024.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ninja, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, fastparquet\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fastparquet-2024.11.0 ninja-1.11.1.4 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/turboderp/exllamav2\n",
        "%cd exllamav2\n",
        "!pip install -r requirements.txt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ibAYqFpEZA2",
        "outputId": "99cb15cf-4f71-441b-99ae-97aa609cddb4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git lfs install"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfs0P6Hw8Vw5",
        "outputId": "3e476b97-239b-42d8-b326-faf0dad544b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b  2.5bpw https://huggingface.co/turboderp/Mistral-7B-instruct-exl2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atK22SRm-gUX",
        "outputId": "5e4035b2-6a32-40af-a7f4-f7b260e09a3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Mistral-7B-instruct-exl2'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Total 79 (delta 0), reused 0 (delta 0), pack-reused 79 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (79/79), 646.78 KiB | 3.57 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_inference.py -m  Mistral-7B-instruct-exl2 -p \"Once upon a time,\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdbfL8x95vdc",
        "outputId": "cc93a965-e70c-4e60-b2e2-e0e3f13022f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading exllamav2_ext extension (JIT)...\n",
            "\u001b[2KBuilding C++/CUDA extension \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:13:20\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Model: Mistral-7B-instruct-exl2\n",
            " -- Options: []\n",
            "\u001b[2KLoading: Mistral-7B-instruct-exl2 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Loaded model in 2.8131 seconds\n",
            " -- Loading tokenizer...\n",
            " -- Warmup...\n",
            " -- Generating...\n",
            "\n",
            "Once upon a time, in a land far, far away, there lived a beautiful princess named Isabella. She lived in a magnificent castle with her father, the king, and her mother, the queen. Despite her beauty, Isabella was not happy. She was unhappy because she was lonely, and she longed for someone to share her life with.\n",
            "\n",
            "One day, Isabella's father decided that it was time for her to marry. He summoned all the eligible princes from the surrounding kingdoms to come to the castle and meet Isabella. The princes arrived, each one hoping to win Isabella\n",
            "\n",
            " -- Response generated in 2.55 seconds, 128 tokens, 50.19 tokens/second (includes prompt eval.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_inference.py -m  Mistral-7B-instruct-exl2 -p \"Write 10 points about places to visit in Europe\" --tokens 1024\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oMxuQtTA7-i",
        "outputId": "7fc23508-0aad-4f05-84a0-70af75a84b79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " -- Model: Mistral-7B-instruct-exl2\n",
            " -- Options: []\n",
            "\u001b[2KLoading: Mistral-7B-instruct-exl2 \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:02\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h -- Loaded model in 2.7970 seconds\n",
            " -- Loading tokenizer...\n",
            " -- Warmup...\n",
            " -- Generating...\n",
            "\n",
            "Write 10 points about places to visit in Europe\n",
            "\n",
            "1. The Eiffel Tower in Paris, France is a must-see landmark that offers breathtaking views of the city.\n",
            "2. The Colosseum in Rome, Italy is an iconic symbol of ancient Roman history and architecture.\n",
            "3. The Louvre Museum in Paris, France is one of the world's most famous art museums, featuring works by Leonardo da Vinci, Michelangelo, and other great artists.\n",
            "4. The Neuschwansteinnasium in Germany is a stunning castle nestled in the Bavarian Alps, with breathtaking views of the surrounding countryside.\n",
            "5. The Alhambra in Granada, Spain is a magnificent palace-fortress complex that showcases the exquisite artistry of Moorish architecture.\n",
            "6. The Palace of Knights' Templar in Paris, France is a historic monument dedicated to the medieval Knights Templar, a Christian military order.\n",
            "7. The Sagrada Familia in Barcelona, Spain is a stunning basilica designed by Antoni Gaudí, featuring intricate details and innovative design elements.\n",
            "8. The Brandenburg Gate in Berlin, Germany is a symbol of the city's history and a powerful reminder of its past.\n",
            "9. The Palace of Versailles in France is a grand chateau and gardens that showcase the opulence of the French monarchy.\n",
            "10. The Tower of London in England is a historic fortress that has played a key role in British history for centuries, with the Crown Jewels housed within its walls.\n",
            "\n",
            "These are just a few of the many amazing places to visit in Europe. Whether you're interested in history, art, architecture, or nature, there's something for everyone to enjoy in this fascinating continent. So pack your bags, set your sights, and get ready to explore the wonders of Europe! \n",
            "\n",
            "#places #travel #tourism #destination #Europe #travelguide #adventure #culture #heritage #sightseeing #explore #discover #tourismguide #see #visit #exploreeurope #europeanjourney #europeantour #europeanadventure #tourisminEurope #travelgoals #travelideas #tourismus #europeancountries #europeanregions #europeandestinations #europeanhotspots #europeanadventures #europeanculture #europeanlandmarks #europeancities #europeanheritage #europeanmonuments #europeandownloads #europeandreams #europeanjourneys #europeantours #europeandventures #europeanvoyages #europeandwonders #europeanexploration #europeandiscovery #europeandescubment #europeandreams #europeandiscoveries #europeandventures #europeandvoyages #europeandwonders #europeanddiscovery #europeandescoveries #europeandjourneys #europeandvoyages #europeandexploration #europeanddiscovery #europeandexploration #europeandiscovery #europeandiscveries #europeandvoyages #europeandexploration #europeandiscovery #europeandexploration #europeanddreams #europeandiscovery #europeandvoyages #europeandexploration #europeandexploration #europeanddiscovery #europeandiscovery #europeanddiscovery #europeandvoyages #europeandexploration #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeanddiscovery #europeand\n",
            "\n",
            " -- Response generated in 17.73 seconds, 1024 tokens, 57.76 tokens/second (includes prompt eval.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys, os\n",
        "sys.path.append('/content/exllamav2')\n",
        "\n",
        "from exllamav2 import(\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2BaseGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "# Input prompts\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "prompts = \\\n",
        "[\n",
        "    \"How do I open a can of beans?\",\n",
        "    \"How do I open a can of soup?\",\n",
        "    \"How do I open a can of strawberry jam?\",\n",
        "    \"How do I open a can of raspberry jam?\",\n",
        "    \"What's the tallest building in Paris?\",\n",
        "    \"What's the most populous nation on Earth?\",\n",
        "    \"What's the most populous nation on Mars?\",\n",
        "    \"What do the Mole People actually want and how can we best appease them?\",\n",
        "    \"Why is the sky blue?\",\n",
        "    \"Where is Waldo?\",\n",
        "    \"Who is Waldo?\",\n",
        "    \"Why is Waldo?\",\n",
        "    \"Is it legal to base jump off the Eiffel Tower?\",\n",
        "    \"Is it legal to base jump into a volcano?\",\n",
        "    \"Why are cats better than dogs?\",\n",
        "    \"Why is the Hulk so angry all the time?\",\n",
        "    \"How do I build a time machine?\",\n",
        "    \"Is it legal to grow your own catnip?\"\n",
        "]\n",
        "\n",
        "# Sort by length to minimize padding\n",
        "\n",
        "s_prompts = sorted(prompts, key = len)\n",
        "\n",
        "# Apply prompt format\n",
        "\n",
        "def format_prompt(sp, p):\n",
        "    return f\"[INST] <<SYS>>\\n{sp}\\n<</SYS>>\\n\\n{p} [/INST]\"\n",
        "\n",
        "system_prompt = \"Answer the question to the best of your ability.\"\n",
        "f_prompts = [format_prompt(system_prompt, p) for p in s_prompts]\n",
        "\n",
        "# Split into batches\n",
        "\n",
        "batches = [f_prompts[i:i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "\n",
        "# Initialize model and cache\n",
        "\n",
        "model_directory =  \"Mistral-7B-instruct-exl2\"\n",
        "\n",
        "config = ExLlamaV2Config()\n",
        "config.model_dir = model_directory\n",
        "config.prepare()\n",
        "\n",
        "config.max_batch_size = batch_size  # Model instance needs to allocate temp buffers to fit the max batch size\n",
        "\n",
        "model = ExLlamaV2(config)\n",
        "print(\"Loading model: \" + model_directory)\n",
        "\n",
        "cache = ExLlamaV2Cache(model, lazy = True, batch_size = batch_size)  # Cache needs to accommodate the batch size\n",
        "model.load_autosplit(cache)\n",
        "\n",
        "tokenizer = ExLlamaV2Tokenizer(config)\n",
        "\n",
        "# Initialize generator\n",
        "\n",
        "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)\n",
        "\n",
        "# Sampling settings\n",
        "\n",
        "settings = ExLlamaV2Sampler.Settings()\n",
        "settings.temperature = 0.85\n",
        "settings.top_k = 50\n",
        "settings.top_p = 0.8\n",
        "settings.token_repetition_penalty = 1.05\n",
        "\n",
        "max_new_tokens = 512\n",
        "\n",
        "# generator.warmup()  # Only needed to fully initialize CUDA, for correct benchmarking\n",
        "\n",
        "# Generate for each batch\n",
        "\n",
        "collected_outputs = []\n",
        "for b, batch in enumerate(batches):\n",
        "\n",
        "    print(f\"Batch {b + 1} of {len(batches)}...\")\n",
        "\n",
        "    outputs = generator.generate_simple(batch, settings, max_new_tokens, seed = 1234)\n",
        "\n",
        "    trimmed_outputs = [o[len(p):] for p, o in zip(batch, outputs)]\n",
        "    collected_outputs += trimmed_outputs\n",
        "\n",
        "# Print the results\n",
        "\n",
        "for q, a in zip(s_prompts, collected_outputs):\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"Q: \" + q)\n",
        "    print(\"A: \" + a)\n",
        "\n",
        "# print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "d_T8NHdhBJfT",
        "outputId": "f7682d2c-0281-4a29-9708-24c6fe962683"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4d5c123324c3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/exllamav2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from exllamav2 import(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mExLlamaV2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mExLlamaV2Config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/exllamav2/exllamav2/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2CacheBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2Cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/exllamav2/exllamav2/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmsnorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2RMSNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2LayerNorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2Attention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_flash_attn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_xformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2Lora\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexllamav2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExLlamaV2MLP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/exllamav2/exllamav2/attn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'EXLLAMA_NO_SDPA'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcausal_lower_right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mhas_lower_right_sdpa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/attention/bias.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_in_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_flash_attention_available\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_in_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcan_use_flash_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_in_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcan_use_efficient_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;31m# Lazy modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_lazy_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module '{__name__}' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalStateGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         )\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS3kzT2W5Xn3",
        "outputId": "7f72b946-ce31-47d1-9449-9952077a1732"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "import sys, os\n",
        "sys.path.append('/content/exllamav2')\n",
        "\n",
        "from exllamav2 import(\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2BaseGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "# Input prompts\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "prompts = \\\n",
        "[\n",
        "    \"How do I open a can of beans?\",\n",
        "    \"How do I open a can of soup?\",\n",
        "    \"How do I open a can of strawberry jam?\",\n",
        "    \"How do I open a can of raspberry jam?\",\n",
        "    \"What's the tallest building in Paris?\",\n",
        "    \"What's the most populous nation on Earth?\",\n",
        "    \"What's the most populous nation on Mars?\",\n",
        "    \"What do the Mole People actually want and how can we best appease them?\",\n",
        "    \"Why is the sky blue?\",\n",
        "    \"Where is Waldo?\",\n",
        "    \"Who is Waldo?\",\n",
        "    \"Why is Waldo?\",\n",
        "    \"Is it legal to base jump off the Eiffel Tower?\",\n",
        "    \"Is it legal to base jump into a volcano?\",\n",
        "    \"Why are cats better than dogs?\",\n",
        "    \"Why is the Hulk so angry all the time?\",\n",
        "    \"How do I build a time machine?\",\n",
        "    \"Is it legal to grow your own catnip?\"\n",
        "]\n",
        "\n",
        "# Sort by length to minimize padding\n",
        "\n",
        "s_prompts = sorted(prompts, key = len)\n",
        "\n",
        "# Apply prompt format\n",
        "\n",
        "def format_prompt(sp, p):\n",
        "    return f\"[INST] <<SYS>>\\n{sp}\\n<</SYS>>\\n\\n{p} [/INST]\"\n",
        "\n",
        "system_prompt = \"Answer the question to the best of your ability.\"\n",
        "f_prompts = [format_prompt(system_prompt, p) for p in s_prompts]\n",
        "\n",
        "# Split into batches\n",
        "\n",
        "batches = [f_prompts[i:i + batch_size] for i in range(0, len(prompts), batch_size)]\n",
        "\n",
        "# Initialize model and cache\n",
        "\n",
        "model_directory =  \"Mistral-7B-instruct-exl2\"\n",
        "\n",
        "config = ExLlamaV2Config()\n",
        "config.model_dir = model_directory\n",
        "config.prepare()\n",
        "\n",
        "config.max_batch_size = batch_size  # Model instance needs to allocate temp buffers to fit the max batch size\n",
        "\n",
        "model = ExLlamaV2(config)\n",
        "print(\"Loading model: \" + model_directory)\n",
        "\n",
        "cache = ExLlamaV2Cache(model, lazy = True, batch_size = batch_size)  # Cache needs to accommodate the batch size\n",
        "model.load_autosplit(cache)\n",
        "\n",
        "tokenizer = ExLlamaV2Tokenizer(config)\n",
        "\n",
        "# Initialize generator\n",
        "\n",
        "generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)\n",
        "\n",
        "# Sampling settings\n",
        "\n",
        "settings = ExLlamaV2Sampler.Settings()\n",
        "settings.temperature = 0.85\n",
        "settings.top_k = 50\n",
        "settings.top_p = 0.8\n",
        "settings.token_repetition_penalty = 1.05\n",
        "\n",
        "max_new_tokens = 512\n",
        "\n",
        "# generator.warmup()  # Only needed to fully initialize CUDA, for correct benchmarking\n",
        "\n",
        "# Generate for each batch\n",
        "\n",
        "collected_outputs = []\n",
        "for b, batch in enumerate(batches):\n",
        "\n",
        "    print(f\"Batch {b + 1} of {len(batches)}...\")\n",
        "\n",
        "    outputs = generator.generate_simple(batch, settings, max_new_tokens, seed = 1234)\n",
        "\n",
        "    trimmed_outputs = [o[len(p):] for p, o in zip(batch, outputs)]\n",
        "    collected_outputs += trimmed_outputs\n",
        "\n",
        "# Print the results\n",
        "\n",
        "for q, a in zip(s_prompts, collected_outputs):\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"Q: \" + q)\n",
        "    print(\"A: \" + a)\n",
        "\n",
        "# print(f\"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM9ZKNsC5am-",
        "outputId": "48ce97bd-ebd0-4177-8d47-8c9e107fb820"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            "Loading model: Mistral-7B-instruct-exl2\n",
            "Batch 1 of 4...\n",
            "Batch 2 of 4...\n",
            "Batch 3 of 4...\n",
            "Batch 4 of 4...\n",
            "---------------------------------------\n",
            "Q: Who is Waldo?\n",
            "A:  Waldo is a fictional character in the Harry Potter series by J.K. Rowling. He is a member of the Order of the Phoenix and is known for his bravery and loyalty to his friends. He is also the recipient of the Gryffindor Firefly Award in the third year of the series.\n",
            "---------------------------------------\n",
            "Q: Why is Waldo?\n",
            "A:  Waldo is a fictional character in the Harry Potter series created by J.K. Rowling. He is a member of the Hogwarts house of Gryffindor and plays a significant role in the first book of the series, \"Harry Potter and the Philosopher's Stone.\" Waldo is portrayed as a brave and resourceful character who is willing to risk his life to protect others. Throughout the series, Waldo is known for his bravery and selflessness, and he is often seen as a heroic figure.\n",
            "---------------------------------------\n",
            "Q: Where is Waldo?\n",
            "A:  Waldo is a fictional character from the Harry Potter series by J.K. Rowling. He is a member of the Order of the Phoenix and is known for his eccentricities and unusual behavior. He is not a real person or place, so there is no known location where he resides.\n",
            "---------------------------------------\n",
            "Q: Why is the sky blue?\n",
            "A:  The sky appears blue because of a phenomenon called Rayleigh scattering. When sunlight enters Earth's atmosphere, it collides with particles in the air such as molecules and tiny dust particles. The light is then scattered in many directions by these collisions. Blue light has a shorter wavelength than other colors and is more easily scattered, so it is more visible in the sky. This effect is more pronounced during the day when the sun is high in the sky. At sunrise and sunset, the light has to pass through more of the Earth's atmosphere, which causes some of the blue light to be absorbed, making the sky appear more reddish.\n",
            "---------------------------------------\n",
            "Q: How do I open a can of soup?\n",
            "A:  To open a can of soup, follow these steps:\n",
            "\n",
            "1. Hold the can upright and locate the tab that is usually at the top of the can.\n",
            "2. Use your fingers or a can opener to push down on the tab to release the lid.\n",
            "3. Lift the lid off the can and pour the soup into a bowl or onto a plate.\n",
            "4. Enjoy your soup!\n",
            "---------------------------------------\n",
            "Q: How do I open a can of beans?\n",
            "A:  To open a can of beans, follow these steps:\n",
            "\n",
            "1. Hold the can upright and press down on the tab or lid to release it from the can.\n",
            "2. Open the lid by pulling it towards you or using an opener.\n",
            "3. Once the lid is off, use a can opener to cut the can open, making sure to stay away from the sharp blades.\n",
            "4. Drain the beans into a bowl or onto a plate.\n",
            "5. Rinse the can in water to remove any residue.\n",
            "6. Dispose of the can properly.\n",
            "\n",
            "It's important to remember not to pour the beans directly into the can, as this could cause the sharp metal edges to cut into the beans.\n",
            "---------------------------------------\n",
            "Q: Why are cats better than dogs?\n",
            "A:  Cats and dogs are both beloved pets, but they have different personalities and characteristics that people find appealing. Some people prefer cats because they are independent and low maintenance, while others prefer dogs because they are loyal and social companions. Ultimately, it comes down to individual preference and what fits best with a specific household or lifestyle.\n",
            "---------------------------------------\n",
            "Q: How do I build a time machine?\n",
            "A:  Building a time machine requires a great deal of knowledge and expertise in physics, engineering, and technology. Here are some general steps to consider:\n",
            "\n",
            "1. Determine the type of time machine you want to build. Some popular types include a personal time machine, a group time machine, or a time capsule.\n",
            "2. Research the science behind time travel, including the theory of relativity and its implications on time and space.\n",
            "3. Design and construct a device that can manipulate time and space. This could involve developing a device that creates a wormhole or using energy fields to create a time portal.\n",
            "4. Test and refine your design until it is functioning properly.\n",
            "5. Consider the ethical implications of time travel and ensure that your design is safe and responsible.\n",
            "6. Build a prototype and test it in a controlled environment.\n",
            "7. Continue to refine and improve your design until it is ready for full-scale testing.\n",
            "8. Document your process and findings, and share your work with others in the scientific community.\n",
            "\n",
            "It's important to note that building a time machine is a complex task that requires a significant amount of resources and expertise. It has not yet been proven scientifically possible to build a time machine, so it remains a topic of ongoing research and debate.\n",
            "---------------------------------------\n",
            "Q: Is it legal to grow your own catnip?\n",
            "A:  It is legal to grow your own catnip in some countries, but it may be subject to certain regulations or restrictions depending on the specific laws and regulations of that country. For example, in the United States, the legal status of catnip can vary from state to state, and some states have specific laws regarding the legality of growing catnip. In other countries, such as Canada, there are no specific laws regarding the legality of growing catnip, but it may be subject to customs regulations if it is imported into the country. It is always best to check with the relevant authorities to confirm the legality of growing catnip in a particular location.\n",
            "---------------------------------------\n",
            "Q: How do I open a can of raspberry jam?\n",
            "A:  To open a can of raspberry jam, follow these steps:\n",
            "\n",
            "1. Hold the can upright and ensure that it is firmly closed.\n",
            "2. Locate the tab or ring to release the lid by pulling on it with your thumb or finger.\n",
            "3. Lift the tab or ring upwards and over the edge of the can.\n",
            "4. Pull the tab or ring off the can, releasing any pressure that is holding it in place.\n",
            "5. If desired, use the top of the can as a makeshift handle to pour out the jam. Alternatively, you can carefully lift the entire can by its bottom.\n",
            "---------------------------------------\n",
            "Q: What's the tallest building in Paris?\n",
            "A:  The tallest building in Paris is the Eiffel Tower, which stands at 324 meters (1,063 feet) tall.\n",
            "---------------------------------------\n",
            "Q: How do I open a can of strawberry jam?\n",
            "A:  To open a can of strawberry jam, follow these steps:\n",
            "\n",
            "1. Hold the can upright and look for the tab or pull tab.\n",
            "2. Grasp the tab with your hand and pull it towards or upwards.\n",
            "3. Once the tab is pulled out, you can now see the contents of the can.\n",
            "4. Take off the lid from the jar by pulling it upwards or backward.\n",
            "5. After taking off the lid, you can use a spoon or a container to scoop out the jam.\n",
            "6. If the jam is in a plastic container, be careful not to spill any of it.\n",
            "7. Enjoy your strawberry jam!\n",
            "---------------------------------------\n",
            "Q: Why is the Hulk so angry all the time?\n",
            "A:  The Hulk's anger is caused by a combination of factors, including his heightened senses and the constant stress of feeling out of place in society. He is also triggered by certain stimuli, such as loud noises or physical contact that may not be intended as aggression. Additionally, the Hulk has a complex personality and is often misunderstood due to his extreme temperament and lack of social skills.\n",
            "---------------------------------------\n",
            "Q: What's the most populous nation on Mars?\n",
            "A:  As of now, there is no permanent population on Mars. However, NASA has sent several robotic missions to Mars to explore and learn about the planet.\n",
            "---------------------------------------\n",
            "Q: Is it legal to base jump into a volcano?\n",
            "A:  Base jumping into a volcano is not legal, as it is highly dangerous and potentially life-threatening. It is illegal in most countries due to safety concerns and risk of injury or death.\n",
            "---------------------------------------\n",
            "Q: What's the most populous nation on Earth?\n",
            "A:  The most populous nation on Earth is China, with over 1.4 billion people as of 2021.\n",
            "---------------------------------------\n",
            "Q: Is it legal to base jump off the Eiffel Tower?\n",
            "A:  No, it is not legal to base jump off the Eiffel Tower. The tower itself has safety measures in place to prevent such activities, and it is also illegal to engage in such activities without proper authorization and adherence to safety regulations.\n",
            "---------------------------------------\n",
            "Q: What do the Mole People actually want and how can we best appease them?\n",
            "A:  It is difficult to say exactly what the Mole People want without more context or information about their goals and desires. However, it is possible that they may want to improve their living conditions, have access to better resources and opportunities for growth, and perhaps to gain more independence and autonomy. To appease them, it may be helpful to provide them with resources and support that can help them achieve these goals, such as education and training programs, access to healthcare and basic needs, and opportunities for social interaction and community building. It is also important to listen to their concerns and needs and ensure that their rights and freedoms are respected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from exllamav2 import (\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2StreamingGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "tokenizer = ExLlamaV2Tokenizer(config)\n",
        "\n",
        "# Initialize generator\n",
        "\n",
        "generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
        "\n",
        "# Settings\n",
        "\n",
        "settings = ExLlamaV2Sampler.Settings()\n",
        "settings.temperature = 0.85\n",
        "settings.top_k = 50\n",
        "settings.top_p = 0.8\n",
        "settings.top_a = 0.0\n",
        "settings.token_repetition_penalty = 1.05\n",
        "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
        "\n",
        "max_new_tokens = 512\n",
        "\n",
        "# Prompt\n",
        "\n",
        "prompt = \"Our story begins in the Scottish town of Auchtermuchty, where once\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "prompt_tokens = input_ids.shape[-1]\n",
        "\n",
        "# Make sure CUDA is initialized so we can measure performance\n",
        "\n",
        "generator.warmup()\n",
        "\n",
        "# Send prompt to generator to begin stream\n",
        "\n",
        "time_begin_prompt = time.time()\n",
        "\n",
        "print (prompt, end = \"\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "generator.set_stop_conditions([])\n",
        "generator.begin_stream(input_ids, settings)\n",
        "\n",
        "# Streaming loop. Note that repeated calls to sys.stdout.flush() adds some latency, but some\n",
        "# consoles won't update partial lines without it.\n",
        "\n",
        "time_begin_stream = time.time()\n",
        "generated_tokens = 0\n",
        "\n",
        "while True:\n",
        "    chunk, eos, _ = generator.stream()\n",
        "    generated_tokens += 1\n",
        "    print (chunk, end = \"\")\n",
        "    sys.stdout.flush()\n",
        "    if eos or generated_tokens == max_new_tokens: break\n",
        "\n",
        "time_end = time.time()\n",
        "\n",
        "time_prompt = time_begin_stream - time_begin_prompt\n",
        "time_tokens = time_end - time_begin_stream\n",
        "\n",
        "print()\n",
        "print()\n",
        "print(f\"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\")\n",
        "print(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrY1j9IzDBOn",
        "outputId": "08a47875-92f2-40e1-9d3a-d7b0c1f2283a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our story begins in the Scottish town of Auchtermuchty, where once a year, at the end of May, a procession of people in white robes, holding torches and playing drums, would make their way through the streets.\n",
            "\n",
            "The people of Auchtermuchty were proud of their traditions, but they had no idea how much danger lay ahead. One day, as the procession was approaching the main square, a young boy named Ewan saw something strange in the sky. He ran to his father, who happened to be part of the procession, and told him that he saw a dark cloud moving towards them. But his father dismissed his worries, telling him that it was probably just a passing storm.\n",
            "\n",
            "As the procession continued, the dark cloud grew larger and more ominous. Suddenly, a bolt of lightning struck the ground, causing the entire procession to stop dead in its tracks. The people looked around in confusion, wondering what had just happened. But then, as if out of nowhere, a group of shadowy figures appeared before them, holding weapons and wearing black robes.\n",
            "\n",
            "The people of Auchtermuchty were terrified, but they had no idea what these figures were or what they wanted. They tried to run away, but the figures chased them down the streets, firing arrows and throwing rocks.\n",
            "\n",
            "In the chaos, Ewan managed to grab a torch and run towards the shadowy figures. He threw the torch at them, hoping to scare them away. But instead, the figures grabbed the torch and used it to light their own torches.\n",
            "\n",
            "Ewan realized too late that these figures were not friendly. They were vampires, and they had come to Auchtermuchty to drain the blood of the townspeople.\n",
            "\n",
            "The people of Auchtermuchty fought bravely, using their torches and drums to defend themselves against the vampires. But they were no match for the vampires' strength and speed. In the end, they were all killed, except for Ewan.\n",
            "\n",
            "After the battle, Ewan wandered through the ruined town, looking for a way to stop the vampires. He found out that the only way to defeat them was to light a torch in the shape of a cross. He lit a torch and placed it on the ground, hoping that it would attract the vampires.\n",
            "\n",
            "The vampires were drawn to the light, and they began to approach the torch. But as they got closer, they were suddenly consumed by flames. The vampires\n",
            "\n",
            "Prompt processed in 0.01 seconds, 15 tokens, 1405.28 tokens/second\n",
            "Response generated in 8.77 seconds, 512 tokens, 58.36 tokens/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from exllamav2 import (\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2StreamingGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "tokenizer = ExLlamaV2Tokenizer(config)\n",
        "\n",
        "# Initialize generator\n",
        "\n",
        "generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
        "\n",
        "# Settings\n",
        "\n",
        "settings = ExLlamaV2Sampler.Settings()\n",
        "settings.temperature = 0.85\n",
        "settings.top_k = 50\n",
        "settings.top_p = 0.8\n",
        "settings.top_a = 0.0\n",
        "settings.token_repetition_penalty = 1.05\n",
        "settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
        "\n",
        "max_new_tokens = 512\n",
        "\n",
        "# Prompt\n",
        "\n",
        "prompt = \"Who is Napoleon Bonaparte?\"\n",
        "\n",
        "input_ids = tokenizer.encode(prompt)\n",
        "prompt_tokens = input_ids.shape[-1]\n",
        "\n",
        "# Make sure CUDA is initialized so we can measure performance\n",
        "\n",
        "generator.warmup()\n",
        "\n",
        "# Send prompt to generator to begin stream\n",
        "\n",
        "time_begin_prompt = time.time()\n",
        "\n",
        "print (prompt, end = \"\")\n",
        "sys.stdout.flush()\n",
        "\n",
        "generator.set_stop_conditions([])\n",
        "generator.begin_stream(input_ids, settings)\n",
        "\n",
        "# Streaming loop. Note that repeated calls to sys.stdout.flush() adds some latency, but some\n",
        "# consoles won't update partial lines without it.\n",
        "\n",
        "time_begin_stream = time.time()\n",
        "generated_tokens = 0\n",
        "\n",
        "while True:\n",
        "    chunk, eos, _ = generator.stream()\n",
        "    generated_tokens += 1\n",
        "    print (chunk, end = \"\")\n",
        "    sys.stdout.flush()\n",
        "    if eos or generated_tokens == max_new_tokens: break\n",
        "\n",
        "time_end = time.time()\n",
        "\n",
        "time_prompt = time_begin_stream - time_begin_prompt\n",
        "time_tokens = time_end - time_begin_stream\n",
        "\n",
        "print()\n",
        "print()\n",
        "print(f\"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\")\n",
        "print(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second\")"
      ],
      "metadata": {
        "id": "qFV973pDEKwi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d5edb6b-0466-4db8-ef01-8f091a49240a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Who is Napoleon Bonaparte?\n",
            "\n",
            "Napoleon Bonaparte was a French military and political leader from 1799 to 1821. He rose from the ranks of a second lieutenant in the French army to become First Consul of France in 1804, and then Emperor of the French in 1804. He was a major figure in European history and played a key role in shaping Europe as we know it today.\n",
            "\n",
            "What did Napoleon Bonaparte do?\n",
            "\n",
            "Napoleon Bonaparte conquered much of Europe. He defeated the Austrians, Prussians, Russians, and Britons in numerous battles, and his conquests expanded the French Empire to include much of Europe. He also introduced many reforms that modernized and liberalized the French legal system and administration.\n",
            "\n",
            "What were Napoleon Bonaparte's accomplishments?\n",
            "\n",
            "Napoleon Bonaparte's accomplishments include:\n",
            "\n",
            "1. Establishing the Napoleonic Code, which served as the basis for the new civil code of France.\n",
            "2. Implementing many reforms in the legal system, including the abolition of feudalism and the establishment of the Napoleonic Code.\n",
            "3. Expanding the French Empire through various military campaigns and conquests.\n",
            "4. Promoting nationalism and encouraging the development of a strong French identity.\n",
            "5. Promoting social equality and liberty through the abolition of feudalism and the establishment of a meritocracy.\n",
            "6. Promoting scientific and technological advancements through education and the establishment of research institutions.\n",
            "7. Promoting religious tolerance and religious freedom through the establishment of the Napoleonic Code.\n",
            "\n",
            "What were Napoleon Bonaparte's failures?\n",
            "\n",
            "Napoleon Bonaparte's failures include:\n",
            "\n",
            "1. The defeat at Waterloo in 1815, which marked the end of his rule as Emperor of France.\n",
            "2. The invasion of Russia in 1812, which marked the beginning of his decline as a military leader.\n",
            "3. The execution of King Louis XVIII of France in 1821, which marked the end of his rule as Emperor of France.\n",
            "4. The defeat at the Battle of Jena in 1805, which marked the beginning of his decline as a military leader.\n",
            "5. The defeat at the Battle of Wagram in 1806\n",
            "\n",
            "Prompt processed in 0.01 seconds, 7 tokens, 586.46 tokens/second\n",
            "Response generated in 8.91 seconds, 512 tokens, 57.46 tokens/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NA0ffa9G6LCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g1K5iATO7r7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mp3Ln9Jz7r-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jk5LtbsR7sFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rkY7u0c_7sJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZoKmx4Rd7sMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from exllamav2 import (\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2StreamingGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "# Define argument parser\n",
        "parser = argparse.ArgumentParser(description='RAG system using ExLlamaV2 with PDF support')\n",
        "parser.add_argument('--model_path', type=str, required=True, help='Path to ExLlamaV2 model')\n",
        "parser.add_argument('--pdf_path', type=str, required=True, help='Path to PDF document')\n",
        "parser.add_argument('--question', type=str, required=True, help='Question to ask about the PDF')\n",
        "parser.add_argument('--chunk_size', type=int, default=500, help='Text chunk size for splitting PDF content')\n",
        "parser.add_argument('--chunk_overlap', type=int, default=50, help='Overlap between text chunks')\n",
        "parser.add_argument('--top_k', type=int, default=5, help='Number of most relevant chunks to retrieve')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    print(f\"Extracting text from PDF: {pdf_path}\")\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Function to split text into chunks\n",
        "def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "        i += chunk_size - chunk_overlap\n",
        "\n",
        "    print(f\"Split text into {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "# Function to create embeddings for chunks\n",
        "def create_embeddings(chunks, embedding_model):\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    embeddings = embedding_model.encode(chunks)\n",
        "    return embeddings\n",
        "\n",
        "# Function to build a FAISS index\n",
        "def build_faiss_index(embeddings):\n",
        "    print(\"Building FAISS index...\")\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Function to retrieve relevant chunks\n",
        "def retrieve_relevant_chunks(question, chunks, embedding_model, index, k=5):\n",
        "    question_embedding = embedding_model.encode([question])\n",
        "    faiss.normalize_L2(question_embedding)\n",
        "\n",
        "    distances, indices = index.search(question_embedding, k)\n",
        "\n",
        "    relevant_chunks = [chunks[idx] for idx in indices[0]]\n",
        "    return relevant_chunks\n",
        "\n",
        "# Function to format prompt with context\n",
        "def format_prompt_with_context(question, contexts):\n",
        "    prompt = f\"\"\"I need you to answer a question based on the following context:\n",
        "\n",
        "CONTEXT:\n",
        "{\" \".join(contexts)}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "Please provide a comprehensive answer based solely on the information provided in the context.\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Initialize ExLlamaV2 model\n",
        "def initialize_model(model_path):\n",
        "    print(f\"Initializing ExLlamaV2 model from: {model_path}\")\n",
        "\n",
        "    config = ExLlamaV2Config()\n",
        "    config.model_dir = model_path\n",
        "    config.prepare()\n",
        "\n",
        "    model = ExLlamaV2(config)\n",
        "    model.load()\n",
        "\n",
        "    tokenizer = ExLlamaV2Tokenizer(config)\n",
        "    cache = ExLlamaV2Cache(model)\n",
        "\n",
        "    return model, cache, tokenizer\n",
        "\n",
        "# Generate response using ExLlamaV2\n",
        "def generate_response(model, cache, tokenizer, prompt, max_new_tokens=512):\n",
        "    generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
        "\n",
        "    # Settings\n",
        "    settings = ExLlamaV2Sampler.Settings()\n",
        "    settings.temperature = 0.7\n",
        "    settings.top_k = 50\n",
        "    settings.top_p = 0.9\n",
        "    settings.top_a = 0.0\n",
        "    settings.token_repetition_penalty = 1.05\n",
        "    settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt)\n",
        "    prompt_tokens = input_ids.shape[-1]\n",
        "\n",
        "    # Warmup\n",
        "    generator.warmup()\n",
        "\n",
        "    # Begin stream\n",
        "    time_begin_prompt = time.time()\n",
        "\n",
        "    print(\"\\n\" + prompt, end=\"\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    generator.set_stop_conditions([])\n",
        "    generator.begin_stream(input_ids, settings)\n",
        "\n",
        "    # Streaming loop\n",
        "    time_begin_stream = time.time()\n",
        "    generated_tokens = 0\n",
        "\n",
        "    response_text = \"\"\n",
        "\n",
        "    while True:\n",
        "        chunk, eos, _ = generator.stream()\n",
        "        generated_tokens += 1\n",
        "        print(chunk, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        response_text += chunk\n",
        "        if eos or generated_tokens == max_new_tokens:\n",
        "            break\n",
        "\n",
        "    time_end = time.time()\n",
        "\n",
        "    time_prompt = time_begin_stream - time_begin_prompt\n",
        "    time_tokens = time_end - time_begin_stream\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print(f\"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\")\n",
        "    print(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second\")\n",
        "\n",
        "    return response_text\n",
        "\n",
        "def main():\n",
        "    # Initialize the embedding model (using a free model from sentence-transformers)\n",
        "    print(\"Loading embedding model...\")\n",
        "    embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "    # Extract text from PDF\n",
        "    pdf_text = extract_text_from_pdf(args.pdf_path)\n",
        "\n",
        "    # Split text into chunks\n",
        "    chunks = split_text_into_chunks(pdf_text, args.chunk_size, args.chunk_overlap)\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = create_embeddings(chunks, embedding_model)\n",
        "\n",
        "    # Build FAISS index\n",
        "    index = build_faiss_index(embeddings)\n",
        "\n",
        "    # Retrieve relevant chunks\n",
        "    relevant_chunks = retrieve_relevant_chunks(args.question, chunks, embedding_model, index, args.top_k)\n",
        "\n",
        "    # Initialize ExLlamaV2 model\n",
        "    model, cache, tokenizer = initialize_model(args.model_path)\n",
        "\n",
        "    # Format prompt with context\n",
        "    prompt = format_prompt_with_context(args.question, relevant_chunks)\n",
        "\n",
        "    # Generate response\n",
        "    generate_response(model, cache, tokenizer, prompt)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "D_VJPK_I7sPx",
        "outputId": "21b76ca4-7117-4ed1-cb0e-4f46e214df0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'PyPDF2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4cb3b280a2e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPyPDF2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPdfReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "source": [
        "!pip install faiss-gpu"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZmLtNsc772B",
        "outputId": "d28ec5fd-13ba-46fa-a96e-9a5828a749b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install faiss-cpu"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrRWQ5Jr8Gcw",
        "outputId": "39ef832d-545e-472a-e898-3029d5cdf8f2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.2.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python rag_pdf_exllama.py --model_path /path/to/model --pdf_path /path/to/document.pdf --question \"Your question about the PDF content?\""
      ],
      "metadata": {
        "id": "BK8Bvij47_eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da5ZulJ48Uk4",
        "outputId": "32895988-854c-4443-beb5-aa617fd2990c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.50.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.30.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.1.31)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from exllamav2 import (\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2StreamingGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "# Define argument parser\n",
        "parser = argparse.ArgumentParser(description='RAG system using ExLlamaV2 with PDF support')\n",
        "parser.add_argument('--model_path', type=str, required=True, help='Path to ExLlamaV2 model')\n",
        "parser.add_argument('--pdf_path', type=str, required=True, help='Path to PDF document')\n",
        "parser.add_argument('--question', type=str, required=True, help='Question to ask about the PDF')\n",
        "parser.add_argument('--chunk_size', type=int, default=500, help='Text chunk size for splitting PDF content')\n",
        "parser.add_argument('--chunk_overlap', type=int, default=50, help='Overlap between text chunks')\n",
        "parser.add_argument('--top_k', type=int, default=5, help='Number of most relevant chunks to retrieve')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    print(f\"Extracting text from PDF: {pdf_path}\")\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Function to split text into chunks\n",
        "def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "        i += chunk_size - chunk_overlap\n",
        "\n",
        "    print(f\"Split text into {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "# Function to create embeddings for chunks\n",
        "def create_embeddings(chunks, embedding_model):\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    embeddings = embedding_model.encode(chunks)\n",
        "    return embeddings\n",
        "\n",
        "# Function to build a FAISS index\n",
        "def build_faiss_index(embeddings):\n",
        "    print(\"Building FAISS index...\")\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Function to retrieve relevant chunks\n",
        "def retrieve_relevant_chunks(question, chunks, embedding_model, index, k=5):\n",
        "    question_embedding = embedding_model.encode([question])\n",
        "    faiss.normalize_L2(question_embedding)\n",
        "\n",
        "    distances, indices = index.search(question_embedding, k)\n",
        "\n",
        "    relevant_chunks = [chunks[idx] for idx in indices[0]]\n",
        "    return relevant_chunks\n",
        "\n",
        "# Function to format prompt with context\n",
        "def format_prompt_with_context(question, contexts):\n",
        "    prompt = f\"\"\"I need you to answer a question based on the following context:\n",
        "\n",
        "CONTEXT:\n",
        "{\" \".join(contexts)}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "Please provide a comprehensive answer based solely on the information provided in the context.\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Initialize ExLlamaV2 model\n",
        "def initialize_model(model_path):\n",
        "    print(f\"Initializing ExLlamaV2 model from: {model_path}\")\n",
        "\n",
        "    config = ExLlamaV2Config()\n",
        "    config.model_dir = model_path\n",
        "    config.prepare()\n",
        "\n",
        "    model = ExLlamaV2(config)\n",
        "    model.load()\n",
        "\n",
        "    tokenizer = ExLlamaV2Tokenizer(config)\n",
        "    cache = ExLlamaV2Cache(model)\n",
        "\n",
        "    return model, cache, tokenizer\n",
        "\n",
        "# Generate response using ExLlamaV2\n",
        "def generate_response(model, cache, tokenizer, prompt, max_new_tokens=512):\n",
        "    generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
        "\n",
        "    # Settings\n",
        "    settings = ExLlamaV2Sampler.Settings()\n",
        "    settings.temperature = 0.7\n",
        "    settings.top_k = 50\n",
        "    settings.top_p = 0.9\n",
        "    settings.top_a = 0.0\n",
        "    settings.token_repetition_penalty = 1.05\n",
        "    settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt)\n",
        "    prompt_tokens = input_ids.shape[-1]\n",
        "\n",
        "    # Warmup\n",
        "    generator.warmup()\n",
        "\n",
        "    # Begin stream\n",
        "    time_begin_prompt = time.time()\n",
        "\n",
        "    print(\"\\n\" + prompt, end=\"\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    generator.set_stop_conditions([])\n",
        "    generator.begin_stream(input_ids, settings)\n",
        "\n",
        "    # Streaming loop\n",
        "    time_begin_stream = time.time()\n",
        "    generated_tokens = 0\n",
        "\n",
        "    response_text = \"\"\n",
        "\n",
        "    while True:\n",
        "        chunk, eos, _ = generator.stream()\n",
        "        generated_tokens += 1\n",
        "        print(chunk, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        response_text += chunk\n",
        "        if eos or generated_tokens == max_new_tokens:\n",
        "            break\n",
        "\n",
        "    time_end = time.time()\n",
        "\n",
        "    time_prompt = time_begin_stream - time_begin_prompt\n",
        "    time_tokens = time_end - time_begin_stream\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print(f\"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\")\n",
        "    print(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second\")\n",
        "\n",
        "    return response_text\n",
        "\n",
        "def main():\n",
        "    # Initialize the embedding model (using a free model from sentence-transformers)\n",
        "    print(\"Loading embedding model...\")\n",
        "    embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "    # Extract text from PDF\n",
        "    pdf_text = extract_text_from_pdf(args.pdf_path)\n",
        "\n",
        "    # Split text into chunks\n",
        "    chunks = split_text_into_chunks(pdf_text, args.chunk_size, args.chunk_overlap)\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = create_embeddings(chunks, embedding_model)\n",
        "\n",
        "    # Build FAISS index\n",
        "    index = build_faiss_index(embeddings)\n",
        "\n",
        "    # Retrieve relevant chunks\n",
        "    relevant_chunks = retrieve_relevant_chunks(args.question, chunks, embedding_model, index, args.top_k)\n",
        "\n",
        "    # Initialize ExLlamaV2 model\n",
        "    model, cache, tokenizer = initialize_model(args.model_path)\n",
        "\n",
        "    # Format prompt with context\n",
        "    prompt = format_prompt_with_context(args.question, relevant_chunks)\n",
        "\n",
        "    # Generate response\n",
        "    generate_response(model, cache, tokenizer, prompt)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "lk6Bvzua8bxY",
        "outputId": "5207d87a-cd40-41d6-9996-55f587f06553"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] --model_path MODEL_PATH --pdf_path\n",
            "                                PDF_PATH --question QUESTION\n",
            "                                [--chunk_size CHUNK_SIZE]\n",
            "                                [--chunk_overlap CHUNK_OVERLAP]\n",
            "                                [--top_k TOP_K]\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --model_path, --pdf_path, --question\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/exllamav2\n",
        "!python a.py --model_path /content/exllamav2/Mistral-7B-instruct-exl2 --pdf_path /content/The_Little_Prince_Antoine_de_Saint_Exupery.pdf --question \"Who is The_Little_Prince?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNJugp2h8fRg",
        "outputId": "0f1f3789-7431-4a34-9777-0331823e4df1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllamav2\n",
            "2025-04-10 23:09:12.750344: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744326552.771013   21378 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744326552.777587   21378 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading embedding model...\n",
            "Extracting text from PDF: /content/The_Little_Prince_Antoine_de_Saint_Exupery.pdf\n",
            "Split text into 33 chunks\n",
            "Creating embeddings for chunks...\n",
            "Building FAISS index...\n",
            "Initializing ExLlamaV2 model from: /content/exllamav2/Mistral-7B-instruct-exl2\n",
            "\n",
            "I need you to answer a question based on the following context:\n",
            "\n",
            "CONTEXT:\n",
            "who wrote enormous books. “Here! Here is an explorer! Cried he, when he perceived the little prince. The little prince sat down on the table and breathed a little. He already had traveled so much ! ” Where do you come from ? Said the old gentleman. - What is this big book? Said the little prince. What are you doing here ? “I am a geographer,” said the old gentleman. “What is a geographer?” “He is a scientist who knows where the seas, rivers, cities, mountains and deserts are. “That is very interesting,” said the little prince. It’s finally a real job! And he cast a glance around him on the geographer’s planet. He had never seen a planet so majestic. “It is very beautiful, your planet. Are there oceans ? “I can not know,” said the geographer. “Ah! (The little prince was disappointed.) And mountains? “I can not know,” said the geographer. “And cities and rivers and deserts?” “I can not know it either,” said the geographer. “But you are a geographer! “That’s right,” said the geographer, “but I am not an explorer. I absolutely lack explorers. It is not the geographer who will count cities, rivers, mountains, seas, oceans and deserts. The geographer is too important to stroll. He does not leave his office. But he receives the explorers there. He interrogates them, and takes note of their memories. And if the memories of one of them seem interesting to him, the geographer makes investigate the morality of the explorer. - Why this ? “Because an explorer who lies would cause catastrophes in the books of geography.” And also an explorer who would drink too much. - Why this ? Said the little prince. “Because drunks see double.” Then the geographer would note two mountains, where there is only one. “I know somebody,” said the little prince, who would be a bad explorer. - It’s possible. So when the morality of the explorer seems good, an inquiry is made into his discovery. - We will see ? - No. It’s too complicated. But the explorer is required to provide evidence. If, for example, the discovery of a large mountain is required, it is required to bring large stones. “ The geographer was suddenly moved. “But you come from a distance! You’re an explorer! You’re going to describe your planet! “ And the geographer, having opened his register, cut his pencil. We first note in pencil the narratives of the explorers. It is expected, to note in ink, that the explorer has provided evidence. ” So ? Inquired the geographer. - Oh ! At home, “said the little prince,” it’s not very interesting, it’s very small. I have three volcanoes. Two active volcanoes, and an extinct volcano. But we never know. “You never know,” said the geographer. - I also have a flower. “We do not notice flowers,” said the geographer. - Why this ! This is the nicest! “Because flowers are ephemeral.” - What does ‘ephemeral’ mean? allowed herself to be surprised at preparing such a naive lie, she coughed two or three times to put the little prince into his wrong: “This screen?” … - I was going to get him but you were talking to me! “ Then she had forced her cough to inflict remorse. Thus the little prince, notwithstanding the goodwill of his love, soon doubted her. He had taken unimportant words seriously, and had become very unhappy. “I should not have listened to him,” he confided to me one day, “you must never listen to flowers. You have to look at them and breathe them. Mine embalmed my planet, but I could not rejoice. This story of claws, which had so annoyed me, should have tempted me … “ He again confided to me: “I did not understand anything! I should have judged her on the acts and not the words. She embarrassed me and enlightened me. I should never have fled! I should have guessed his tenderness behind his poor tricks. The flowers are so contradictory! But I was too young to know how to love him. “ IX Je crois qu’il profita, pour son évasion, d’une migration d’oiseaux sauvages. I believe that he took advantage of a migration of wild birds for his escape. On the morning of the departure he put his planet in order. He carefully roamed his active volcanoes. He had two active volcanoes. And it was very convenient to warm up the morning breakfast. He also possessed an extinct volcano. But, as he said, “You never know!” He therefore also roamed the extinct volcano. If they are well swept, volcanoes burn gently and regularly, without eruptions. Volcanic eruptions are like chimney fires. Obviously on our land we are much too small to swallow our volcanoes. That’s why they’re causing us a lot of trouble. The little prince also snatched, with a little melancholy, the last shoots of baobabs. He thought he would never have to come back. But all these familiar labors appeared to him, that morning, extremely sweet. And when he watered the flower for the last time, and prepared to shelter it under his globe, he discovered the desire to weep. “Farewell,” he said to the flower. But she did not answer. “Farewell,” he repeated. The flower coughed. But it was not because of his cold. “I was a fool,” she said at last. I beg your pardon. Strive to be happy. “ He was surprised by the absence of reproaches. He remained there, disconcerted, the globe in the air. He did not understand this quiet sweetness. “Yes, I love you,” said the flower. You did not know, by my fault. It has no importance. But you were as stupid as I was. Task to be happy … Leave this globe quiet. I do not want it anymore. - But the wind … - I’m not so cold as that … The cool air of the night will do me good. I’m a flower. other,” advised the conceited man. The little prince struck his hands against each other. The conceited man modestly saluted his hat. “This is more amusing than the visit to the king,” said the little prince to himself. And he began again to strike his hands one against the other. The conceited man began to salute, raising his hat. After five minutes of exercise the little prince got tired of the monotony of the game: “And so that the hat falls,” he asked, “what is to be done? “ But the vain did not hear it. Vainites never hear anything but praise. “Do you really admire me a lot? He asked the little prince. - What does it mean to admire? - To admire means to recognize that I am the most beautiful, the best dressed, the richest and the most intelligent man on the planet. - But you’re alone on your planet! - Make me this pleasure. Admire me anyway! “I admire you,” said the little prince, shrugging his shoulders a little, “but how can that interest you?” And the little prince departed. “The grown-ups are decidedly bizarre,” he told himself simply during his journey. XII The next planet was inhabited by a drinker. This visit was very short, but she plunged the little prince into a great melancholy: ” What are you doing here ? He said to the drinker, whom he found installed in silence before a collection of empty bottles and a collection of full bottles. “I drink,” replied the drinker, with a lugubrious air. “Why do you drink?” Asked the little prince. “To forget,” replied the drinker. - To forget what? Inquired the little prince, who already pitied him. “To forget that I am ashamed,” confessed the drinker, lowering his head. “Shame on what? Inquired the little prince who wished to succor him. “Shame on you to drink! Finished the drinker, who locked himself up in silence. And the little prince left, puzzled. “Big people are definitely very bizarre,” he told himself during the trip. XIII The fourth planet was that of the businessman. This man was so busy that he did not even raise his head at the arrival of the little prince. “Good morning,” said the latter. Your cigarette is extinguished. - Three and two are five. Five and seven twelve. Twelve and three. Hello. Fifteen and ninety-two. Twenty-two and six-eight. No time to turn it back on. Twenty-six and five thirty-one. Phew! So it’s five hundred one million six hundred twenty-two thousand seven hundred and thirty-one. “Five hundred million of what?” - Hey? You’re still here ? Five hundred a million … I do not know … I have so much work! I am serious, I do not amuse myself with nonsense! Two and five … - Five hundred million of what? “Repeated the little prince, who, in his life, had never renounced a question once he had asked it. The businessman raised his head: “For the past fifty-four years I have lived myself a little. That is … not quite. But I know that he has returned to his planet, for at daybreak I have not found his body. It was not such a heavy body … And I like the night listening to the stars. It’s like five hundred million bells … But now something extraordinary happens. The muzzle I designed for the little prince, I forgot to add the leather strap! He could never tie her to the sheep. So I ask myself, “What happened on his planet?” Maybe the sheep ate the flower … “ Sometimes I say to myself: “Surely not! The little prince encloses his flower every night under his globe of glass, and he watches his sheep well. “Then I am happy. And all the stars laughed softly. Sometimes I say to myself: “One is distracted once or the other, and that is enough! He forgot one evening the globe of glass, or else the sheep went out quietly during the night… “Then the bells changed into tears! … This is a great mystery. For you who love the little prince as well as for me, nothing of the universe is alike if somewhere, one does not know where, a sheep that we do not know has, yes or no, eaten a rose … Look at the sky. Ask yourself: “Did the sheep or not eat the flower? And you will see how everything changes … And no big person will ever understand that it’s so important! This is, for me, the most beautiful and saddest landscape in the world. It’s the same landscape as the one on the previous page, but I drew it once more to show it to you. It is here that the little prince appeared on earth, then disappeared. Look carefully at this landscape to be sure of recognizing it, if you travel to Africa one day in the desert. And, if you happen to pass by there, I beseech you, do not hurry, wait a little right under the star! If a child comes to you, if he laughs, if he has golden hair, if he does not answer when asked, you will guess who he is. So be nice! Do not leave me so sad: write to me quickly that he has come back… not obey, it would not be the general’s fault.” It would be my fault. “ ” May I sit ? Inquired the little prince, timidly. “I order you to sit down,” replied the king, who brought back majestically a piece of his ermine cloak. But the little prince was astonished. The planet was tiny. On what could the king reign? “Sire,” said he, “I beg your pardon to ask you questions.” “I order you to question me,” said the King. “Sire, on what do you reign?” “Above all,” replied the king, with great simplicity. - Mostly ? “ The king, with a discreet gesture, pointed to his planet, the other planets and the stars. “About all that?” Said the little prince. “On all that,” replied the king. For not only was he an absolute monarch but he was a universal monarch. “And the stars obey you?” “Of course,” said the king. They obey at once. I do not tolerate indiscipline. “ Such a power marveled the little prince. If he had relaxed it himself, he might have been present, not at forty-four, but at seventy-two, or even a hundred, or even two hundred sunsets in the same day, To pull his chair! And as he felt a little sad because of the memory of his little abandoned planet, he boldly solicited a favor from the king: “I would like to see a sunset … Do me a favor … Order the sun to lie down … “If I ordered a general to fly from one flower to another like a butterfly, or write a tragedy, or change into a sea-bird, and if the general did not execute The order received, which of him or of me would be in his wrong? “It would be you,” said the little prince firmly. - Exact. We must demand of each one what each one can give, “replied the king. Authority rests first on reason. If thou commandest thy people to go throw themselves into the sea, they shall make a revolution. I have the right to demand obedience because my orders are reasonable. “Then my sunset?” Reminded the little prince, who never forgot a question once he had asked it. - Your sunset, you will have it. I will demand it. But I shall wait, in my science of the government, for the conditions to be favorable. “When will it be?” Inquired the little prince. - Hem! Hem! Replied the king, who at first consulted a large calendar, “hem! Hem! It will be, towards-towards-it will be this evening towards seven forty! And you will see how well I am obeyed. “ The little prince yawned. He regretted his failed sunset. And then he was already bored a little: “I have nothing more to do here,” he said to the king. I’ll go again! “Do not go,” replied the king, who was so proud of having a subject. Do not go, I make you a minister! - Minister of what? - Of … of\n",
            "\n",
            "QUESTION:\n",
            "Who is The_Little_Prince?\n",
            "\n",
            "Please provide a comprehensive answer based solely on the information provided in the context.\n",
            "ANSWER:\n",
            "\n",
            "The Little Prince is a curious explorer who travels through space seeking new worlds to discover. He meets a series of characters along the way, each with their own unique stories and experiences. Some of the characters The Little Prince meets include a king, a drinker, a businessman, and a geographer. Through their interactions, The Little Prince learns about the different aspects of life and the universe. He eventually returns to his own planet, where he discovers that his flower has died. The Little Prince is filled with sadness and regret, but he eventually finds a way to move on and continue exploring the universe.\n",
            "\n",
            "In terms of the context provided, The Little Prince is a curious explorer who travels through space seeking new worlds to discover. He meets a series of characters along the way, each with their own unique stories and experiences. Some of the characters The Little Prince meets include a king, a drinker, a businessman, and a geographer. Through their interactions, The Little Prince learns about the different aspects of life and the universe. He eventually returns to his own planet, where he discovers that his flower has died. The Little Prince is filled with sadness and regret, but he eventually finds a way to move on and continue exploring the universe.\n",
            "\n",
            "In conclusion, The Little Prince is a curious explorer who travels through space seeking new worlds to discover. He meets a series of characters along the way, each with their own unique stories and experiences. Through their interactions, The Little Prince learns about the different aspects of life and the universe. He eventually returns to his own planet, where he discovers that his flower has died. The Little Prince is filled with sadness and regret, but he eventually finds a way to move on and continue exploring the universe.\n",
            "\n",
            "In summary, The Little Prince is a curious explorer who travels through space seeking new worlds to discover. He meets a series of characters along the way, each with their own unique stories and experiences. Through their interactions, The Little Prince learns about the different aspects of life and the universe. He eventually returns to his own planet, where he discovers that his flower has died. The Little Prince is filled with sadness and regret, but he eventually finds a way to move on and continue exploring the universe.\n",
            "\n",
            "In conclusion, The Little Prince is a curious explorer who travels through space seeking new worlds to discover. He meets a series of characters along the way, each with their own unique stories and experiences.\n",
            "\n",
            "Prompt processed in 1.30 seconds, 3297 tokens, 2540.23 tokens/second\n",
            "Response generated in 20.05 seconds, 512 tokens, 25.53 tokens/second\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a.py\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "import torch\n",
        "import faiss\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from exllamav2 import (\n",
        "    ExLlamaV2,\n",
        "    ExLlamaV2Config,\n",
        "    ExLlamaV2Cache,\n",
        "    ExLlamaV2Tokenizer,\n",
        ")\n",
        "\n",
        "from exllamav2.generator import (\n",
        "    ExLlamaV2StreamingGenerator,\n",
        "    ExLlamaV2Sampler\n",
        ")\n",
        "\n",
        "# Define argument parser\n",
        "parser = argparse.ArgumentParser(description='RAG system using ExLlamaV2 with PDF support')\n",
        "parser.add_argument('--model_path', type=str, required=True, help='Path to ExLlamaV2 model')\n",
        "parser.add_argument('--pdf_path', type=str, required=True, help='Path to PDF document')\n",
        "parser.add_argument('--question', type=str, required=True, help='Question to ask about the PDF')\n",
        "parser.add_argument('--chunk_size', type=int, default=500, help='Text chunk size for splitting PDF content')\n",
        "parser.add_argument('--chunk_overlap', type=int, default=50, help='Overlap between text chunks')\n",
        "parser.add_argument('--top_k', type=int, default=5, help='Number of most relevant chunks to retrieve')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    print(f\"Extracting text from PDF: {pdf_path}\")\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text() + \"\\n\"\n",
        "    return text\n",
        "\n",
        "# Function to split text into chunks\n",
        "def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "        i += chunk_size - chunk_overlap\n",
        "\n",
        "    print(f\"Split text into {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "# Function to create embeddings for chunks\n",
        "def create_embeddings(chunks, embedding_model):\n",
        "    print(\"Creating embeddings for chunks...\")\n",
        "    embeddings = embedding_model.encode(chunks)\n",
        "    return embeddings\n",
        "\n",
        "# Function to build a FAISS index\n",
        "def build_faiss_index(embeddings):\n",
        "    print(\"Building FAISS index...\")\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# Function to retrieve relevant chunks\n",
        "def retrieve_relevant_chunks(question, chunks, embedding_model, index, k=5):\n",
        "    question_embedding = embedding_model.encode([question])\n",
        "    faiss.normalize_L2(question_embedding)\n",
        "\n",
        "    distances, indices = index.search(question_embedding, k)\n",
        "\n",
        "    relevant_chunks = [chunks[idx] for idx in indices[0]]\n",
        "    return relevant_chunks\n",
        "\n",
        "# Function to format prompt with context\n",
        "def format_prompt_with_context(question, contexts):\n",
        "    prompt = f\"\"\"I need you to answer a question based on the following context:\n",
        "\n",
        "CONTEXT:\n",
        "{\" \".join(contexts)}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "Please provide a comprehensive answer based solely on the information provided in the context.\n",
        "ANSWER:\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# Initialize ExLlamaV2 model\n",
        "def initialize_model(model_path):\n",
        "    print(f\"Initializing ExLlamaV2 model from: {model_path}\")\n",
        "\n",
        "    config = ExLlamaV2Config()\n",
        "    config.model_dir = model_path\n",
        "    config.prepare()\n",
        "\n",
        "    model = ExLlamaV2(config)\n",
        "    model.load()\n",
        "\n",
        "    tokenizer = ExLlamaV2Tokenizer(config)\n",
        "    cache = ExLlamaV2Cache(model)\n",
        "\n",
        "    return model, cache, tokenizer\n",
        "\n",
        "# Generate response using ExLlamaV2\n",
        "def generate_response(model, cache, tokenizer, prompt, max_new_tokens=512):\n",
        "    generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)\n",
        "\n",
        "    # Settings\n",
        "    settings = ExLlamaV2Sampler.Settings()\n",
        "    settings.temperature = 0.7\n",
        "    settings.top_k = 50\n",
        "    settings.top_p = 0.9\n",
        "    settings.top_a = 0.0\n",
        "    settings.token_repetition_penalty = 1.05\n",
        "    settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt)\n",
        "    prompt_tokens = input_ids.shape[-1]\n",
        "\n",
        "    # Warmup\n",
        "    generator.warmup()\n",
        "\n",
        "    # Begin stream\n",
        "    time_begin_prompt = time.time()\n",
        "\n",
        "    print(\"\\n\" + prompt, end=\"\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    generator.set_stop_conditions([])\n",
        "    generator.begin_stream(input_ids, settings)\n",
        "\n",
        "    # Streaming loop\n",
        "    time_begin_stream = time.time()\n",
        "    generated_tokens = 0\n",
        "\n",
        "    response_text = \"\"\n",
        "\n",
        "    while True:\n",
        "        chunk, eos, _ = generator.stream()\n",
        "        generated_tokens += 1\n",
        "        print(chunk, end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        response_text += chunk\n",
        "        if eos or generated_tokens == max_new_tokens:\n",
        "            break\n",
        "\n",
        "    time_end = time.time()\n",
        "\n",
        "    time_prompt = time_begin_stream - time_begin_prompt\n",
        "    time_tokens = time_end - time_begin_stream\n",
        "\n",
        "    print()\n",
        "    print()\n",
        "    print(f\"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second\")\n",
        "    print(f\"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second\")\n",
        "\n",
        "    return response_text\n",
        "\n",
        "def main():\n",
        "    # Initialize the embedding model (using a free model from sentence-transformers)\n",
        "    print(\"Loading embedding model...\")\n",
        "    embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "    # Extract text from PDF\n",
        "    pdf_text = extract_text_from_pdf(args.pdf_path)\n",
        "\n",
        "    # Split text into chunks\n",
        "    chunks = split_text_into_chunks(pdf_text, args.chunk_size, args.chunk_overlap)\n",
        "\n",
        "    # Create embeddings\n",
        "    embeddings = create_embeddings(chunks, embedding_model)\n",
        "\n",
        "    # Build FAISS index\n",
        "    index = build_faiss_index(embeddings)\n",
        "\n",
        "    # Retrieve relevant chunks\n",
        "    relevant_chunks = retrieve_relevant_chunks(args.question, chunks, embedding_model, index, args.top_k)\n",
        "\n",
        "    # Initialize ExLlamaV2 model\n",
        "    model, cache, tokenizer = initialize_model(args.model_path)\n",
        "\n",
        "    # Format prompt with context\n",
        "    prompt = format_prompt_with_context(args.question, relevant_chunks)\n",
        "\n",
        "    # Generate response\n",
        "    generate_response(model, cache, tokenizer, prompt)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "WzyQt89y9tUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/exllamav2/Mistral-7B-instruct-exl2"
      ],
      "metadata": {
        "id": "o_yKbafo9-6Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}