# -*- coding: utf-8 -*-
"""suc_ask_PDF_exllamav2model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1miPfpetPZ7G-ty5dc4XVAIbnvZPUYZ7i
"""



# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/turboderp/exllamav2
# %cd exllamav2
!pip install -r requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/exllamav2

!git lfs install

!git clone -b  2.5bpw https://huggingface.co/turboderp/Mistral-7B-instruct-exl2

!python test_inference.py -m  Mistral-7B-instruct-exl2 -p "Once upon a time,"

!python test_inference.py -m  Mistral-7B-instruct-exl2 -p "Write 10 points about places to visit in Europe" --tokens 1024

import sys, os
sys.path.append('/content/exllamav2')

from exllamav2 import(
    ExLlamaV2,
    ExLlamaV2Config,
    ExLlamaV2Cache,
    ExLlamaV2Tokenizer,
)

from exllamav2.generator import (
    ExLlamaV2BaseGenerator,
    ExLlamaV2Sampler
)

import time

# Input prompts

batch_size = 5

prompts = \
[
    "How do I open a can of beans?",
    "How do I open a can of soup?",
    "How do I open a can of strawberry jam?",
    "How do I open a can of raspberry jam?",
    "What's the tallest building in Paris?",
    "What's the most populous nation on Earth?",
    "What's the most populous nation on Mars?",
    "What do the Mole People actually want and how can we best appease them?",
    "Why is the sky blue?",
    "Where is Waldo?",
    "Who is Waldo?",
    "Why is Waldo?",
    "Is it legal to base jump off the Eiffel Tower?",
    "Is it legal to base jump into a volcano?",
    "Why are cats better than dogs?",
    "Why is the Hulk so angry all the time?",
    "How do I build a time machine?",
    "Is it legal to grow your own catnip?"
]

# Sort by length to minimize padding

s_prompts = sorted(prompts, key = len)

# Apply prompt format

def format_prompt(sp, p):
    return f"[INST] <<SYS>>\n{sp}\n<</SYS>>\n\n{p} [/INST]"

system_prompt = "Answer the question to the best of your ability."
f_prompts = [format_prompt(system_prompt, p) for p in s_prompts]

# Split into batches

batches = [f_prompts[i:i + batch_size] for i in range(0, len(prompts), batch_size)]

# Initialize model and cache

model_directory =  "Mistral-7B-instruct-exl2"

config = ExLlamaV2Config()
config.model_dir = model_directory
config.prepare()

config.max_batch_size = batch_size  # Model instance needs to allocate temp buffers to fit the max batch size

model = ExLlamaV2(config)
print("Loading model: " + model_directory)

cache = ExLlamaV2Cache(model, lazy = True, batch_size = batch_size)  # Cache needs to accommodate the batch size
model.load_autosplit(cache)

tokenizer = ExLlamaV2Tokenizer(config)

# Initialize generator

generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)

# Sampling settings

settings = ExLlamaV2Sampler.Settings()
settings.temperature = 0.85
settings.top_k = 50
settings.top_p = 0.8
settings.token_repetition_penalty = 1.05

max_new_tokens = 512

# generator.warmup()  # Only needed to fully initialize CUDA, for correct benchmarking

# Generate for each batch

collected_outputs = []
for b, batch in enumerate(batches):

    print(f"Batch {b + 1} of {len(batches)}...")

    outputs = generator.generate_simple(batch, settings, max_new_tokens, seed = 1234)

    trimmed_outputs = [o[len(p):] for p, o in zip(batch, outputs)]
    collected_outputs += trimmed_outputs

# Print the results

for q, a in zip(s_prompts, collected_outputs):
    print("---------------------------------------")
    print("Q: " + q)
    print("A: " + a)

# print(f"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second")

!pip install --upgrade numpy

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/exllamav2
import sys, os
sys.path.append('/content/exllamav2')

from exllamav2 import(
    ExLlamaV2,
    ExLlamaV2Config,
    ExLlamaV2Cache,
    ExLlamaV2Tokenizer,
)

from exllamav2.generator import (
    ExLlamaV2BaseGenerator,
    ExLlamaV2Sampler
)

import time

# Input prompts

batch_size = 5

prompts = \
[
    "How do I open a can of beans?",
    "How do I open a can of soup?",
    "How do I open a can of strawberry jam?",
    "How do I open a can of raspberry jam?",
    "What's the tallest building in Paris?",
    "What's the most populous nation on Earth?",
    "What's the most populous nation on Mars?",
    "What do the Mole People actually want and how can we best appease them?",
    "Why is the sky blue?",
    "Where is Waldo?",
    "Who is Waldo?",
    "Why is Waldo?",
    "Is it legal to base jump off the Eiffel Tower?",
    "Is it legal to base jump into a volcano?",
    "Why are cats better than dogs?",
    "Why is the Hulk so angry all the time?",
    "How do I build a time machine?",
    "Is it legal to grow your own catnip?"
]

# Sort by length to minimize padding

s_prompts = sorted(prompts, key = len)

# Apply prompt format

def format_prompt(sp, p):
    return f"[INST] <<SYS>>\n{sp}\n<</SYS>>\n\n{p} [/INST]"

system_prompt = "Answer the question to the best of your ability."
f_prompts = [format_prompt(system_prompt, p) for p in s_prompts]

# Split into batches

batches = [f_prompts[i:i + batch_size] for i in range(0, len(prompts), batch_size)]

# Initialize model and cache

model_directory =  "Mistral-7B-instruct-exl2"

config = ExLlamaV2Config()
config.model_dir = model_directory
config.prepare()

config.max_batch_size = batch_size  # Model instance needs to allocate temp buffers to fit the max batch size

model = ExLlamaV2(config)
print("Loading model: " + model_directory)

cache = ExLlamaV2Cache(model, lazy = True, batch_size = batch_size)  # Cache needs to accommodate the batch size
model.load_autosplit(cache)

tokenizer = ExLlamaV2Tokenizer(config)

# Initialize generator

generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)

# Sampling settings

settings = ExLlamaV2Sampler.Settings()
settings.temperature = 0.85
settings.top_k = 50
settings.top_p = 0.8
settings.token_repetition_penalty = 1.05

max_new_tokens = 512

# generator.warmup()  # Only needed to fully initialize CUDA, for correct benchmarking

# Generate for each batch

collected_outputs = []
for b, batch in enumerate(batches):

    print(f"Batch {b + 1} of {len(batches)}...")

    outputs = generator.generate_simple(batch, settings, max_new_tokens, seed = 1234)

    trimmed_outputs = [o[len(p):] for p, o in zip(batch, outputs)]
    collected_outputs += trimmed_outputs

# Print the results

for q, a in zip(s_prompts, collected_outputs):
    print("---------------------------------------")
    print("Q: " + q)
    print("A: " + a)

# print(f"Response generated in {time_total:.2f} seconds, {max_new_tokens} tokens, {max_new_tokens / time_total:.2f} tokens/second")

from exllamav2 import (
    ExLlamaV2,
    ExLlamaV2Config,
    ExLlamaV2Cache,
    ExLlamaV2Tokenizer,
)

from exllamav2.generator import (
    ExLlamaV2StreamingGenerator,
    ExLlamaV2Sampler
)

import time


tokenizer = ExLlamaV2Tokenizer(config)

# Initialize generator

generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)

# Settings

settings = ExLlamaV2Sampler.Settings()
settings.temperature = 0.85
settings.top_k = 50
settings.top_p = 0.8
settings.top_a = 0.0
settings.token_repetition_penalty = 1.05
settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])

max_new_tokens = 512

# Prompt

prompt = "Our story begins in the Scottish town of Auchtermuchty, where once"

input_ids = tokenizer.encode(prompt)
prompt_tokens = input_ids.shape[-1]

# Make sure CUDA is initialized so we can measure performance

generator.warmup()

# Send prompt to generator to begin stream

time_begin_prompt = time.time()

print (prompt, end = "")
sys.stdout.flush()

generator.set_stop_conditions([])
generator.begin_stream(input_ids, settings)

# Streaming loop. Note that repeated calls to sys.stdout.flush() adds some latency, but some
# consoles won't update partial lines without it.

time_begin_stream = time.time()
generated_tokens = 0

while True:
    chunk, eos, _ = generator.stream()
    generated_tokens += 1
    print (chunk, end = "")
    sys.stdout.flush()
    if eos or generated_tokens == max_new_tokens: break

time_end = time.time()

time_prompt = time_begin_stream - time_begin_prompt
time_tokens = time_end - time_begin_stream

print()
print()
print(f"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second")
print(f"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second")

from exllamav2 import (
    ExLlamaV2,
    ExLlamaV2Config,
    ExLlamaV2Cache,
    ExLlamaV2Tokenizer,
)

from exllamav2.generator import (
    ExLlamaV2StreamingGenerator,
    ExLlamaV2Sampler
)

import time


tokenizer = ExLlamaV2Tokenizer(config)

# Initialize generator

generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)

# Settings

settings = ExLlamaV2Sampler.Settings()
settings.temperature = 0.85
settings.top_k = 50
settings.top_p = 0.8
settings.top_a = 0.0
settings.token_repetition_penalty = 1.05
settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])

max_new_tokens = 512

# Prompt

prompt = "Who is Napoleon Bonaparte?"

input_ids = tokenizer.encode(prompt)
prompt_tokens = input_ids.shape[-1]

# Make sure CUDA is initialized so we can measure performance

generator.warmup()

# Send prompt to generator to begin stream

time_begin_prompt = time.time()

print (prompt, end = "")
sys.stdout.flush()

generator.set_stop_conditions([])
generator.begin_stream(input_ids, settings)

# Streaming loop. Note that repeated calls to sys.stdout.flush() adds some latency, but some
# consoles won't update partial lines without it.

time_begin_stream = time.time()
generated_tokens = 0

while True:
    chunk, eos, _ = generator.stream()
    generated_tokens += 1
    print (chunk, end = "")
    sys.stdout.flush()
    if eos or generated_tokens == max_new_tokens: break

time_end = time.time()

time_prompt = time_begin_stream - time_begin_prompt
time_tokens = time_end - time_begin_stream

print()
print()
print(f"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second")
print(f"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second")













import os
import sys
import time
import argparse
import torch
import faiss
import numpy as np
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer

from exllamav2 import (
    ExLlamaV2,
    ExLlamaV2Config,
    ExLlamaV2Cache,
    ExLlamaV2Tokenizer,
)

from exllamav2.generator import (
    ExLlamaV2StreamingGenerator,
    ExLlamaV2Sampler
)

# Define argument parser
parser = argparse.ArgumentParser(description='RAG system using ExLlamaV2 with PDF support')
parser.add_argument('--model_path', type=str, required=True, help='Path to ExLlamaV2 model')
parser.add_argument('--pdf_path', type=str, required=True, help='Path to PDF document')
parser.add_argument('--question', type=str, required=True, help='Question to ask about the PDF')
parser.add_argument('--chunk_size', type=int, default=500, help='Text chunk size for splitting PDF content')
parser.add_argument('--chunk_overlap', type=int, default=50, help='Overlap between text chunks')
parser.add_argument('--top_k', type=int, default=5, help='Number of most relevant chunks to retrieve')
args = parser.parse_args()

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    print(f"Extracting text from PDF: {pdf_path}")
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text() + "\n"
    return text

# Function to split text into chunks
def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):
    words = text.split()
    chunks = []

    i = 0
    while i < len(words):
        chunk = ' '.join(words[i:i + chunk_size])
        chunks.append(chunk)
        i += chunk_size - chunk_overlap

    print(f"Split text into {len(chunks)} chunks")
    return chunks

# Function to create embeddings for chunks
def create_embeddings(chunks, embedding_model):
    print("Creating embeddings for chunks...")
    embeddings = embedding_model.encode(chunks)
    return embeddings

# Function to build a FAISS index
def build_faiss_index(embeddings):
    print("Building FAISS index...")
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    faiss.normalize_L2(embeddings)
    index.add(embeddings)
    return index

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(question, chunks, embedding_model, index, k=5):
    question_embedding = embedding_model.encode([question])
    faiss.normalize_L2(question_embedding)

    distances, indices = index.search(question_embedding, k)

    relevant_chunks = [chunks[idx] for idx in indices[0]]
    return relevant_chunks

# Function to format prompt with context
def format_prompt_with_context(question, contexts):
    prompt = f"""I need you to answer a question based on the following context:

CONTEXT:
{" ".join(contexts)}

QUESTION:
{question}

Please provide a comprehensive answer based solely on the information provided in the context.
ANSWER:
"""
    return prompt

# Initialize ExLlamaV2 model
def initialize_model(model_path):
    print(f"Initializing ExLlamaV2 model from: {model_path}")

    config = ExLlamaV2Config()
    config.model_dir = model_path
    config.prepare()

    model = ExLlamaV2(config)
    model.load()

    tokenizer = ExLlamaV2Tokenizer(config)
    cache = ExLlamaV2Cache(model)

    return model, cache, tokenizer

# Generate response using ExLlamaV2
def generate_response(model, cache, tokenizer, prompt, max_new_tokens=512):
    generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)

    # Settings
    settings = ExLlamaV2Sampler.Settings()
    settings.temperature = 0.7
    settings.top_k = 50
    settings.top_p = 0.9
    settings.top_a = 0.0
    settings.token_repetition_penalty = 1.05
    settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])

    input_ids = tokenizer.encode(prompt)
    prompt_tokens = input_ids.shape[-1]

    # Warmup
    generator.warmup()

    # Begin stream
    time_begin_prompt = time.time()

    print("\n" + prompt, end="")
    sys.stdout.flush()

    generator.set_stop_conditions([])
    generator.begin_stream(input_ids, settings)

    # Streaming loop
    time_begin_stream = time.time()
    generated_tokens = 0

    response_text = ""

    while True:
        chunk, eos, _ = generator.stream()
        generated_tokens += 1
        print(chunk, end="")
        sys.stdout.flush()
        response_text += chunk
        if eos or generated_tokens == max_new_tokens:
            break

    time_end = time.time()

    time_prompt = time_begin_stream - time_begin_prompt
    time_tokens = time_end - time_begin_stream

    print()
    print()
    print(f"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second")
    print(f"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second")

    return response_text

def main():
    # Initialize the embedding model (using a free model from sentence-transformers)
    print("Loading embedding model...")
    embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

    # Extract text from PDF
    pdf_text = extract_text_from_pdf(args.pdf_path)

    # Split text into chunks
    chunks = split_text_into_chunks(pdf_text, args.chunk_size, args.chunk_overlap)

    # Create embeddings
    embeddings = create_embeddings(chunks, embedding_model)

    # Build FAISS index
    index = build_faiss_index(embeddings)

    # Retrieve relevant chunks
    relevant_chunks = retrieve_relevant_chunks(args.question, chunks, embedding_model, index, args.top_k)

    # Initialize ExLlamaV2 model
    model, cache, tokenizer = initialize_model(args.model_path)

    # Format prompt with context
    prompt = format_prompt_with_context(args.question, relevant_chunks)

    # Generate response
    generate_response(model, cache, tokenizer, prompt)

if __name__ == "__main__":
    main()

!pip install faiss-gpu

!pip install faiss-cpu

python rag_pdf_exllama.py --model_path /path/to/model --pdf_path /path/to/document.pdf --question "Your question about the PDF content?"

!pip install PyPDF2 sentence_transformers

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/exllamav2
import os
import sys
import time
import argparse
import torch
import faiss
import numpy as np
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer

from exllamav2 import (
    ExLlamaV2,
    ExLlamaV2Config,
    ExLlamaV2Cache,
    ExLlamaV2Tokenizer,
)

from exllamav2.generator import (
    ExLlamaV2StreamingGenerator,
    ExLlamaV2Sampler
)

# Define argument parser
parser = argparse.ArgumentParser(description='RAG system using ExLlamaV2 with PDF support')
parser.add_argument('--model_path', type=str, required=True, help='Path to ExLlamaV2 model')
parser.add_argument('--pdf_path', type=str, required=True, help='Path to PDF document')
parser.add_argument('--question', type=str, required=True, help='Question to ask about the PDF')
parser.add_argument('--chunk_size', type=int, default=500, help='Text chunk size for splitting PDF content')
parser.add_argument('--chunk_overlap', type=int, default=50, help='Overlap between text chunks')
parser.add_argument('--top_k', type=int, default=5, help='Number of most relevant chunks to retrieve')
args = parser.parse_args()

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    print(f"Extracting text from PDF: {pdf_path}")
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text() + "\n"
    return text

# Function to split text into chunks
def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):
    words = text.split()
    chunks = []

    i = 0
    while i < len(words):
        chunk = ' '.join(words[i:i + chunk_size])
        chunks.append(chunk)
        i += chunk_size - chunk_overlap

    print(f"Split text into {len(chunks)} chunks")
    return chunks

# Function to create embeddings for chunks
def create_embeddings(chunks, embedding_model):
    print("Creating embeddings for chunks...")
    embeddings = embedding_model.encode(chunks)
    return embeddings

# Function to build a FAISS index
def build_faiss_index(embeddings):
    print("Building FAISS index...")
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    faiss.normalize_L2(embeddings)
    index.add(embeddings)
    return index

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(question, chunks, embedding_model, index, k=5):
    question_embedding = embedding_model.encode([question])
    faiss.normalize_L2(question_embedding)

    distances, indices = index.search(question_embedding, k)

    relevant_chunks = [chunks[idx] for idx in indices[0]]
    return relevant_chunks

# Function to format prompt with context
def format_prompt_with_context(question, contexts):
    prompt = f"""I need you to answer a question based on the following context:

CONTEXT:
{" ".join(contexts)}

QUESTION:
{question}

Please provide a comprehensive answer based solely on the information provided in the context.
ANSWER:
"""
    return prompt

# Initialize ExLlamaV2 model
def initialize_model(model_path):
    print(f"Initializing ExLlamaV2 model from: {model_path}")

    config = ExLlamaV2Config()
    config.model_dir = model_path
    config.prepare()

    model = ExLlamaV2(config)
    model.load()

    tokenizer = ExLlamaV2Tokenizer(config)
    cache = ExLlamaV2Cache(model)

    return model, cache, tokenizer

# Generate response using ExLlamaV2
def generate_response(model, cache, tokenizer, prompt, max_new_tokens=512):
    generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)

    # Settings
    settings = ExLlamaV2Sampler.Settings()
    settings.temperature = 0.7
    settings.top_k = 50
    settings.top_p = 0.9
    settings.top_a = 0.0
    settings.token_repetition_penalty = 1.05
    settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])

    input_ids = tokenizer.encode(prompt)
    prompt_tokens = input_ids.shape[-1]

    # Warmup
    generator.warmup()

    # Begin stream
    time_begin_prompt = time.time()

    print("\n" + prompt, end="")
    sys.stdout.flush()

    generator.set_stop_conditions([])
    generator.begin_stream(input_ids, settings)

    # Streaming loop
    time_begin_stream = time.time()
    generated_tokens = 0

    response_text = ""

    while True:
        chunk, eos, _ = generator.stream()
        generated_tokens += 1
        print(chunk, end="")
        sys.stdout.flush()
        response_text += chunk
        if eos or generated_tokens == max_new_tokens:
            break

    time_end = time.time()

    time_prompt = time_begin_stream - time_begin_prompt
    time_tokens = time_end - time_begin_stream

    print()
    print()
    print(f"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second")
    print(f"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second")

    return response_text

def main():
    # Initialize the embedding model (using a free model from sentence-transformers)
    print("Loading embedding model...")
    embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

    # Extract text from PDF
    pdf_text = extract_text_from_pdf(args.pdf_path)

    # Split text into chunks
    chunks = split_text_into_chunks(pdf_text, args.chunk_size, args.chunk_overlap)

    # Create embeddings
    embeddings = create_embeddings(chunks, embedding_model)

    # Build FAISS index
    index = build_faiss_index(embeddings)

    # Retrieve relevant chunks
    relevant_chunks = retrieve_relevant_chunks(args.question, chunks, embedding_model, index, args.top_k)

    # Initialize ExLlamaV2 model
    model, cache, tokenizer = initialize_model(args.model_path)

    # Format prompt with context
    prompt = format_prompt_with_context(args.question, relevant_chunks)

    # Generate response
    generate_response(model, cache, tokenizer, prompt)

if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/exllamav2
!python a.py --model_path /content/exllamav2/Mistral-7B-instruct-exl2 --pdf_path /content/The_Little_Prince_Antoine_de_Saint_Exupery.pdf --question "Who is The_Little_Prince?"

a.py

import os
import sys
import time
import argparse
import torch
import faiss
import numpy as np
from PyPDF2 import PdfReader
from sentence_transformers import SentenceTransformer

from exllamav2 import (
    ExLlamaV2,
    ExLlamaV2Config,
    ExLlamaV2Cache,
    ExLlamaV2Tokenizer,
)

from exllamav2.generator import (
    ExLlamaV2StreamingGenerator,
    ExLlamaV2Sampler
)

# Define argument parser
parser = argparse.ArgumentParser(description='RAG system using ExLlamaV2 with PDF support')
parser.add_argument('--model_path', type=str, required=True, help='Path to ExLlamaV2 model')
parser.add_argument('--pdf_path', type=str, required=True, help='Path to PDF document')
parser.add_argument('--question', type=str, required=True, help='Question to ask about the PDF')
parser.add_argument('--chunk_size', type=int, default=500, help='Text chunk size for splitting PDF content')
parser.add_argument('--chunk_overlap', type=int, default=50, help='Overlap between text chunks')
parser.add_argument('--top_k', type=int, default=5, help='Number of most relevant chunks to retrieve')
args = parser.parse_args()

# Function to extract text from PDF
def extract_text_from_pdf(pdf_path):
    print(f"Extracting text from PDF: {pdf_path}")
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text() + "\n"
    return text

# Function to split text into chunks
def split_text_into_chunks(text, chunk_size=500, chunk_overlap=50):
    words = text.split()
    chunks = []

    i = 0
    while i < len(words):
        chunk = ' '.join(words[i:i + chunk_size])
        chunks.append(chunk)
        i += chunk_size - chunk_overlap

    print(f"Split text into {len(chunks)} chunks")
    return chunks

# Function to create embeddings for chunks
def create_embeddings(chunks, embedding_model):
    print("Creating embeddings for chunks...")
    embeddings = embedding_model.encode(chunks)
    return embeddings

# Function to build a FAISS index
def build_faiss_index(embeddings):
    print("Building FAISS index...")
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    faiss.normalize_L2(embeddings)
    index.add(embeddings)
    return index

# Function to retrieve relevant chunks
def retrieve_relevant_chunks(question, chunks, embedding_model, index, k=5):
    question_embedding = embedding_model.encode([question])
    faiss.normalize_L2(question_embedding)

    distances, indices = index.search(question_embedding, k)

    relevant_chunks = [chunks[idx] for idx in indices[0]]
    return relevant_chunks

# Function to format prompt with context
def format_prompt_with_context(question, contexts):
    prompt = f"""I need you to answer a question based on the following context:

CONTEXT:
{" ".join(contexts)}

QUESTION:
{question}

Please provide a comprehensive answer based solely on the information provided in the context.
ANSWER:
"""
    return prompt

# Initialize ExLlamaV2 model
def initialize_model(model_path):
    print(f"Initializing ExLlamaV2 model from: {model_path}")

    config = ExLlamaV2Config()
    config.model_dir = model_path
    config.prepare()

    model = ExLlamaV2(config)
    model.load()

    tokenizer = ExLlamaV2Tokenizer(config)
    cache = ExLlamaV2Cache(model)

    return model, cache, tokenizer

# Generate response using ExLlamaV2
def generate_response(model, cache, tokenizer, prompt, max_new_tokens=512):
    generator = ExLlamaV2StreamingGenerator(model, cache, tokenizer)

    # Settings
    settings = ExLlamaV2Sampler.Settings()
    settings.temperature = 0.7
    settings.top_k = 50
    settings.top_p = 0.9
    settings.top_a = 0.0
    settings.token_repetition_penalty = 1.05
    settings.disallow_tokens(tokenizer, [tokenizer.eos_token_id])

    input_ids = tokenizer.encode(prompt)
    prompt_tokens = input_ids.shape[-1]

    # Warmup
    generator.warmup()

    # Begin stream
    time_begin_prompt = time.time()

    print("\n" + prompt, end="")
    sys.stdout.flush()

    generator.set_stop_conditions([])
    generator.begin_stream(input_ids, settings)

    # Streaming loop
    time_begin_stream = time.time()
    generated_tokens = 0

    response_text = ""

    while True:
        chunk, eos, _ = generator.stream()
        generated_tokens += 1
        print(chunk, end="")
        sys.stdout.flush()
        response_text += chunk
        if eos or generated_tokens == max_new_tokens:
            break

    time_end = time.time()

    time_prompt = time_begin_stream - time_begin_prompt
    time_tokens = time_end - time_begin_stream

    print()
    print()
    print(f"Prompt processed in {time_prompt:.2f} seconds, {prompt_tokens} tokens, {prompt_tokens / time_prompt:.2f} tokens/second")
    print(f"Response generated in {time_tokens:.2f} seconds, {generated_tokens} tokens, {generated_tokens / time_tokens:.2f} tokens/second")

    return response_text

def main():
    # Initialize the embedding model (using a free model from sentence-transformers)
    print("Loading embedding model...")
    embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')

    # Extract text from PDF
    pdf_text = extract_text_from_pdf(args.pdf_path)

    # Split text into chunks
    chunks = split_text_into_chunks(pdf_text, args.chunk_size, args.chunk_overlap)

    # Create embeddings
    embeddings = create_embeddings(chunks, embedding_model)

    # Build FAISS index
    index = build_faiss_index(embeddings)

    # Retrieve relevant chunks
    relevant_chunks = retrieve_relevant_chunks(args.question, chunks, embedding_model, index, args.top_k)

    # Initialize ExLlamaV2 model
    model, cache, tokenizer = initialize_model(args.model_path)

    # Format prompt with context
    prompt = format_prompt_with_context(args.question, relevant_chunks)

    # Generate response
    generate_response(model, cache, tokenizer, prompt)

if __name__ == "__main__":
    main()

/content/exllamav2/Mistral-7B-instruct-exl2